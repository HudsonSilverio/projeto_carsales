# WebScraping
<label>Acessando a internet, coletando os dados e apresentando-os</label>

<div align=center>
<img src="https://user-images.githubusercontent.com/109561598/197368807-89056542-fbeb-430d-a4c7-dd05c9330790.jpg" width=800 alt=imagem logo do projeto" />
</div>

<!-- ************************************* T√≠tulo ********************************************* -->
<h1 align="center"> Coelta com Scrapy dados Merdado Livre Carros Usados </h1>

<!-- ************************************* Baadges ********************************************* -->

## üöÄ Sobre o Projeto

Caso de mercado:  O cliente  tem uma revendedora de carros  gostaria de realizar uma an√°lise dos pre√ßos de ve√≠culos para ajustar sua estrat√©gia de price na sua concession√°ria e realizar a previs√£o de tend√™ncia de pre√ßos. 
      Seu objetivo √© comparar os pre√ßos de seus ve√≠culos com os pre√ßos de ve√≠culos semelhantes no Mercado Livre, considerando o mesmo ano, modelo e quilometragem e quais s√£o os anos de fabrica√ß√£o, modelos e faixas de quilometragem mais frequentemente listados no Mercado Livre.

## Etapa 01 - Fazendo o setup da para integra√ß√£o com o github

  <li> instalado a vers√£o escolhida - pyenv local 3.12.1</li>

## Etapa 02 - Criando um ambiente isolado para o projeto
  
  <li>
    python -m venv .venv (criando o ambiente);</li>
  <li>
   source .venv/Scripts/activate (ativando o ambiente)
;</li>

## Etapa 03 - Criando um arquivo .gitignore
  <li>
    Crie esse tipo de arquivo no vscode.  Esse arquivo ir√° armazenar tudo que eu n√£o quero que fique salvo no meu git</li>
  

## Etapa 04 - Integrando seu projeto com o github
 <li>     
    ‚ÜíPrimeiro, crie um reposit√≥rio no GITHUB  !   Atrav√©s dos c√≥digos que est√£o na p√°gina no projeto no github, vamos copiar e colar os c√≥digos no bash da p√°gina no vscode .

  <li>
   Copie e cole todos os codigos que aparecem na apagina inicial de criacao do repositorio no terminal ;</li>
  
        
## Etapa 05 - Coletando os Dados

 <li> Instalar o scrapy - pip install scrapy

  <li> A biblioteca Scrapy √© uma ferramenta open-source em Python usada para fazer web scraping, ou seja, para extrair dados de websites de forma autom√°tica

<li> Execute esse codigo no terminal: scrapy startproject coleta ( esse comando cria uma pasta com toda a divis√£o do seu projeto de web scraping)

<li> Crie uma pasta chamada src de certa forma que a pasta coleta esteja dentro dela

<li> scrapy genspider "nome_do _seu_arquivo", ' URL do site " ‚Üí (este comando inicia a coleta no site que for direcionado)

<li> Realizando a coleta dos dados atrav√©s do scrapy shell ( entra dentro de um terminal e faz a requisi√ß√£o no site e navegar dentro do html antes de desenvolver o c√≥digo ) - muito util para testar os cod antes de ir para o arquivo principal.

<li> Caso der o erro 403 e na mensagem de erro aparecer pedindo o USER AGENT, iremos simular um usu√°rio para nao ser barrado pelo site colocando o USER AGENT da sua maquina: pesquise por 'agent user my' no Google e coloque dentro do arquivo 'settings.py' e digite dentro de USER_AGENT = e coloque  o agente user da sua m√°quina. (n√£o esque√ßa das aspas)

<li> Saia do terminal e rode o c√≥digo no bash scrapy shell

<li> Abre o terminal, fa√ßa o comando fetch("URL do site") - se aparecer o cod 200 correu tudo certo(acesso permitido)

<li> Digite o c√≥digo no terminal 'response.text', para poder retornar no terminal  para voc√™ todo o HTML da p√°gina

<li> ‚Üí quando esse c√≥digo √© executado ele cria dentro da pasta spiders um arquivo py com o nome que voc√™ colocou (ex: mercadolivre) - nesse porte que ser√° respons√°vel por fazer os c√≥digos para coletar as informa√ß√µes do site.

 ## Etapa 06 - Inspe√ß√£o Visual dentro do HTML

 <li> Agora nessa etapa de inspecionar no HTML da p√°gina como est√£o as informa√ß√µes que voc√™ deseja coletar. Deve-se procurar visualmente qual a parte que se deseja coletar no c√≥digo html. Dica: clicar onde exatamente na p√°gina onde se quer coletar o dado e inspecionar a p√°gina com o bot√£o direito conforme as figuras 1 e 2 abaixo.

 



